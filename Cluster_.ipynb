import numpy as np
from sklearn.cluster import KMeans
from transformers import BertTokenizer, BertModel
import torch
import nltk
from nltk.tokenize import sent_tokenize
from profanity_filter import ProfanityFilter

# Download NLTK data
nltk.download('punkt')

# Load BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Initialize profanity filter
profanity_filter = ProfanityFilter()

def is_valid_sentence(sentence):
    # Check if sentence is too short or too long
    if len(sentence.split()) < 3 or len(sentence.split()) > 50:
        return False
    
    # Check for profanity
    if profanity_filter.is_profane(sentence):
        return False

    return True

def get_bert_embeddings(sentences):
    # Tokenize and get embeddings from BERT
    with torch.no_grad():
        inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors="pt", max_length=512)
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1)

def sample_diverse_utterances(utterances, sample_size):
    # Implement logic to sample a diverse set of utterances
    # Example: simple random sampling
    return np.random.choice(utterances, size=sample_size, replace=False)

def cluster_utterances(utterances, num_clusters, sample_size=None):
    # Filter and preprocess the utterances
    valid_utterances = [u for u in utterances if is_valid_sentence(u)]

    # Sample a diverse set of utterances if sample_size is provided
    if sample_size is not None and sample_size < len(valid_utterances):
        valid_utterances = sample_diverse_utterances(valid_utterances, sample_size)

    # Get BERT embeddings
    embeddings = get_bert_embeddings(valid_utterances)

    # Clustering
    kmeans = KMeans(n_clusters=num_clusters, random_state=42)
    kmeans.fit(embeddings)

    # Mapping utterances to clusters
    labels = kmeans.labels_
    clustered_utterances = {i: [] for i in range(num_clusters)}
    for utterance, label in zip(valid_utterances, labels):
        clustered_utterances[label].append(utterance)

    return clustered_utterances

# Example usage
utterances = ["Your list of utterances goes here..."]
num_clusters = 5  # Adjust based on your needs
sample_size = 100  # Number of utterances to sample for diversity
clusters = cluster_utterances(utterances, num_clusters, sample_size)

# Output the clusters
for cluster_id, cluster in clusters.items():
    print(f"Cluster {cluster_id}:")
    for utterance in cluster:
        print(f" - {utterance}")
